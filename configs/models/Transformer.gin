import gin.torch.external_configurables
import icu_benchmarks.models.wrappers
import icu_benchmarks.models.encoders
import icu_benchmarks.models.utils
import icu_benchmarks.data.loader

preprocess.use_features = False

# Train params
train_common.model = @DLWrapper()

DLWrapper.encoder = @Transformer()
DLWrapper.optimizer_fn = @Adam

DLWrapper.train.epochs = 1000
DLWrapper.train.batch_size = 64
DLWrapper.train.patience = 10
DLWrapper.train.min_delta = 1e-4

# Optimizer params
Adam.weight_decay = 1e-6
optimizer/hyperparameter.class_to_tune = @Adam
optimizer/hyperparameter.lr = (3e-4, 1e-5)

# Encoder params
Transformer.emb = %EMB
Transformer.ff_hidden_mult = 2
Transformer.l1_reg = 0.0
Transformer.num_classes = %NUM_CLASSES
model/hyperparameter.class_to_tune = @Transformer
model/hyperparameter.hidden = (32, 256)
model/hyperparameter.heads = (1, 8)
model/hyperparameter.depth = (1, 3)
model/hyperparameter.dropout = (0.0, 0.4)
model/hyperparameter.dropout_att = (0.0, 0.4)

tune_hyperparameters.scopes = ["model", "optimizer"]
tune_hyperparameters.init_points = 5
tune_hyperparameters.n_iter = 30
tune_hyperparameters.folds_to_tune_on = 2
tune_hyperparameters.cast_to_int = ["Transformer.hidden", "Transformer.heads", "Transformer.depth"]
