import gin.torch.external_configurables
import icu_benchmarks.models.wrappers
import icu_benchmarks.models.encoders
import icu_benchmarks.models.utils

preprocess.use_features = False

# Train params
train_common.model = @DLWrapper()
train_common.do_test = True

DLWrapper.encoder = @TemporalConvNet()
DLWrapper.optimizer_fn = @Adam

DLWrapper.train.epochs = 1000
DLWrapper.train.batch_size = 64
DLWrapper.train.patience = 10
DLWrapper.train.min_delta = 1e-4

# Optimizer params
optimizer/hyperparameter.class_to_tune = @Adam
optimizer/hyperparameter.weight_decay = 1e-6
optimizer/hyperparameter.lr = (3e-4, 1e-5)

# Encoder params
model/hyperparameter.class_to_tune = @TemporalConvNet
model/hyperparameter.cast_to_int = ["num_channels", "kernel_size"]
model/hyperparameter.num_inputs = %EMB
model/hyperparameter.num_classes = %NUM_CLASSES
model/hyperparameter.max_seq_length = %HORIZON
model/hyperparameter.num_channels = (32, 256)
model/hyperparameter.kernel_size = (2, 32)
model/hyperparameter.dropout = (0.0, 0.4)

tune_hyperparameters.scopes = ["model", "optimizer"]
tune_hyperparameters.init_points = 5
tune_hyperparameters.n_iter = 30
tune_hyperparameters.folds_to_tune_on = 2